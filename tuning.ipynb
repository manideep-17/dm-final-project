{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install imbalanced-learn\n",
    "!pip install imbalanced-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install xgboost\n",
    "!pip install lightgbm\n",
    "!pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing Libraries\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model Selection and Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000) \n",
    "# Set pandas options to display all elements of the array\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Ensure no column truncation\n",
    "pd.set_option('display.max_colwidth', None)  # Display full content in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets & transaction and identity datasets on 'TransactionID'\n",
    "# TRAIN\n",
    "train_transaction = pd.read_csv('/Users/manideepreddyaliminati/Documents/ASUCourses/CSE572/notebook/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/Users/manideepreddyaliminati/Documents/ASUCourses/CSE572/notebook/train_identity.csv')\n",
    "\n",
    "#TEST\n",
    "test_transaction = pd.read_csv('/Users/manideepreddyaliminati/Documents/ASUCourses/CSE572/notebook/test_transaction.csv')\n",
    "test_identity = pd.read_csv('/Users/manideepreddyaliminati/Documents/ASUCourses/CSE572/notebook/test_identity.csv')\n",
    "\n",
    "submission_template = pd.read_csv(\"submission_template.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Transaction Data:\")\n",
    "display(train_transaction.head(5))\n",
    "\n",
    "print(\"Train Identity Data:\")\n",
    "display(train_identity.head(5))\n",
    "\n",
    "print(\"test Transaction Data:\")\n",
    "display(test_transaction.head(5))\n",
    "\n",
    "print(\"test Identity Data:\")\n",
    "display(test_identity.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [col for col in train_transaction.columns if col.startswith('V')]\n",
    "train_transaction = train_transaction.drop(columns=columns_to_remove)\n",
    "test_transaction = test_transaction.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns with missing values and their counts\n",
    "missing_cols = train_transaction.isnull().sum()\n",
    "missing_cols = missing_cols[missing_cols > 0]  # Filter only columns with missing values\n",
    "\n",
    "# Display the columns with missing values and the percentage of missing data\n",
    "missing_percentage = (missing_cols / len(train_transaction)) * 100\n",
    "missing_data = pd.DataFrame({'Missing Values': missing_cols, 'Percentage': missing_percentage})\n",
    "\n",
    "print(\"Columns with Missing Values:\")\n",
    "display(missing_data.sort_values(by='Percentage', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns with missing values and their counts\n",
    "missing_cols = test_transaction.isnull().sum()\n",
    "missing_cols = missing_cols[missing_cols > 0]  # Filter only columns with missing values\n",
    "\n",
    "# Display the columns with missing values and the percentage of missing data\n",
    "missing_percentage = (missing_cols / len(test_transaction)) * 100\n",
    "missing_data = pd.DataFrame({'Missing Values': missing_cols, 'Percentage': missing_percentage})\n",
    "\n",
    "print(\"Columns with Missing Values:\")\n",
    "display(missing_data.sort_values(by='Percentage', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['hour'] = (train_transaction['TransactionDT'] // 3600) % 24  # Hour of the day (0-23)\n",
    "train_transaction['day'] = train_transaction['TransactionDT'] // (3600 * 24)   # Relative day count\n",
    "\n",
    "# Extract day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\n",
    "train_transaction['day_of_week'] = train_transaction['day'] % 7\n",
    "\n",
    "# Apply the same to test data\n",
    "train_transaction['hour'] = (train_transaction['TransactionDT'] // 3600) % 24\n",
    "train_transaction['day'] = train_transaction['TransactionDT'] // (3600 * 24)\n",
    "train_transaction['day_of_week'] = train_transaction['day'] % 7\n",
    "\n",
    "test_transaction['hour'] = (test_transaction['TransactionDT'] // 3600) % 24  # Hour of the day (0-23)\n",
    "test_transaction['day'] = test_transaction['TransactionDT'] // (3600 * 24)   # Relative day count\n",
    "\n",
    "# Extract day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\n",
    "test_transaction['day_of_week'] = test_transaction['day'] % 7\n",
    "\n",
    "# Apply the same to test data\n",
    "test_transaction['hour'] = (test_transaction['TransactionDT'] // 3600) % 24\n",
    "test_transaction['day'] = test_transaction['TransactionDT'] // (3600 * 24)\n",
    "test_transaction['day_of_week'] = test_transaction['day'] % 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['day_bin'] = pd.cut(train_transaction['day'], bins=3, labels=False)\n",
    "test_transaction['day_bin'] = pd.cut(test_transaction['day'], bins=3, labels=False)\n",
    "\n",
    "# test_data['day_bin'] = pd.cut(test_data['day'], bins=100, labels=False)\n",
    "\n",
    "# Calculate fraud percentage for each day of the week\n",
    "fraud_rate_dow = train_transaction.groupby('day_of_week')['isFraud'].mean() * 100\n",
    "\n",
    "# Plot fraud percentage by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=fraud_rate_dow.index, y=fraud_rate_dow, palette='muted')\n",
    "plt.title('Percentage of Fraud Transactions by Day of the Week')\n",
    "plt.xlabel('Day of the Week (0 = Monday, 6 = Sunday)')\n",
    "plt.ylabel('Fraud Percentage (%)')\n",
    "plt.show()\n",
    "\n",
    "# Calculate fraud percentage for each day bin\n",
    "fraud_rate_day_bin = train_transaction.groupby('day_bin')['isFraud'].mean() * 100\n",
    "\n",
    "# Plot fraud percentage by day bin\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.lineplot(x=fraud_rate_day_bin.index, y=fraud_rate_day_bin, marker='o', linestyle='-', palette='Set1')\n",
    "plt.title('Percentage of Fraud Transactions by Day Bins (100 Bins)')\n",
    "plt.xlabel('Day Bin')\n",
    "plt.ylabel('Fraud Percentage (%)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate fraud percentage for each hour of the day\n",
    "fraud_rate_hour = train_transaction.groupby('hour')['isFraud'].mean() * 100\n",
    "\n",
    "# Plot fraud percentage by hour\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=fraud_rate_hour.index, y=fraud_rate_hour, palette='coolwarm')\n",
    "plt.title('Percentage of Fraud Transactions by Hour of the Day')\n",
    "plt.xlabel('Hour of the Day (0-23)')\n",
    "plt.ylabel('Fraud Percentage (%)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min = train_transaction['TransactionAmt'].min()\n",
    "train_max = train_transaction['TransactionAmt'].max()\n",
    "\n",
    "test_min = test_transaction['TransactionAmt'].min()\n",
    "test_max = test_transaction['TransactionAmt'].max()\n",
    "\n",
    "overall_min = min(train_min, test_min)\n",
    "overall_max = max(train_max, test_max)\n",
    "\n",
    "overall_min, overall_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins =   [0,15,      50,        75,         200,         1000,          3000,       5000,    40000]\n",
    "labels = ['Micro', 'Small', 'Medium', 'Large-Low', 'Large-Mid', 'Large-Upper', 'High-Low', 'High-Upper']\n",
    "\n",
    "train_transaction['TransactionAmt_bin'] = pd.cut(\n",
    "    train_transaction['TransactionAmt'], bins=bins, labels=labels, include_lowest=True\n",
    ")\n",
    "test_transaction['TransactionAmt_bin'] = pd.cut(\n",
    "    test_transaction['TransactionAmt'], bins=bins, labels=labels, include_lowest=True\n",
    ")\n",
    "\n",
    "print(train_transaction['TransactionAmt_bin'].value_counts())\n",
    "print(test_transaction['TransactionAmt_bin'].value_counts())\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply encoding on train and test\n",
    "train_transaction['TransactionAmt_bin_encoded'] = label_encoder.fit_transform(train_transaction['TransactionAmt_bin'])\n",
    "test_transaction['TransactionAmt_bin_encoded'] = label_encoder.transform(test_transaction['TransactionAmt_bin'])\n",
    "\n",
    "# Check the encoded values\n",
    "print(train_transaction[['TransactionAmt_bin', 'TransactionAmt_bin_encoded']].head())\n",
    "print(test_transaction[['TransactionAmt_bin', 'TransactionAmt_bin_encoded']].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of C features\n",
    "c_feat = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7',\n",
    "          'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\n",
    "\n",
    "# Compute the absolute correlation matrix for C features\n",
    "corr_matrix = train_transaction[c_feat].corr().abs()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, square=True)\n",
    "plt.title('Correlation Heatmap of Selected Features')\n",
    "plt.show()\n",
    "\n",
    "# Select the upper triangle of the correlation matrix to avoid duplicate pairs\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# List all pairs with high correlation (e.g., correlation > 0.9)\n",
    "high_corr_pairs = [(col1, col2, upper_triangle.loc[col1, col2]) \n",
    "                   for col1 in upper_triangle.columns \n",
    "                   for col2 in upper_triangle.index \n",
    "                   if upper_triangle.loc[col1, col2] > 0.9]\n",
    "\n",
    "# Display the pairs with high correlation\n",
    "print(\"Highly Correlated Column Pairs (C1 to C14, correlation > 0.9):\")\n",
    "for col1, col2, corr in high_corr_pairs:\n",
    "    print(f\"{col1} - {col2}: {corr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_drop_high_corr_columns(data, features, threshold=0.9):\n",
    "    # Compute the absolute correlation matrix for the given features\n",
    "    corr_matrix = data[features].corr().abs()\n",
    "    \n",
    "    # Select the upper triangle of the correlation matrix\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find columns to drop (keep only the first occurrence in correlated pairs)\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    \n",
    "    return to_drop\n",
    "\n",
    "# List of C features\n",
    "c_feat = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7',\n",
    "          'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\n",
    "\n",
    "# Get the list of columns to drop\n",
    "columns_to_drop = auto_drop_high_corr_columns(train_transaction, c_feat, threshold=0.9)\n",
    "\n",
    "print(columns_to_drop)\n",
    "\n",
    "train_transaction.drop(columns=[col for col in columns_to_drop if col in train_transaction], inplace=True)\n",
    "test_transaction.drop(columns=[col for col in columns_to_drop if col in test_transaction], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage = train_transaction.isnull().mean() * 100\n",
    "\n",
    "print(missing_percentage)\n",
    "\n",
    "# Drop features with >50% missing values\n",
    "features_to_drop = missing_percentage[missing_percentage > 50].index.tolist()\n",
    "print(f\"Features to Drop: {features_to_drop}\")\n",
    "train_transaction.drop(columns=[col for col in features_to_drop if col in train_transaction], inplace=True)\n",
    "test_transaction.drop(columns=[col for col in features_to_drop if col in test_transaction], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fill D1-D15 (Timedeltas) with mean or 0 if missing means no previous transaction\n",
    "for col in ['D1', 'D2', 'D3', 'D4', 'D10', 'D11', 'D15']:\n",
    "    train_transaction[col].fillna(train_transaction[col].mean(), inplace=True)\n",
    "    test_transaction[col].fillna(test_transaction[col].mean(), inplace=True)\n",
    "\n",
    "# 2. Fill M1-M9 (Binary/Match Features) with the mode (most frequent value)\n",
    "for col in ['M1', 'M2', 'M3', 'M4', 'M6']:\n",
    "    train_transaction[col].fillna(train_transaction[col].mode()[0], inplace=True)\n",
    "    test_transaction[col].fillna(test_transaction[col].mode()[0], inplace=True)\n",
    "\n",
    "# 3. Fill Card1-Card6 with the mode or label as 'unknown'\n",
    "for col in ['card2', 'card3', 'card4', 'card5', 'card6']:\n",
    "    train_transaction[col].fillna(train_transaction[col].mode()[0], inplace=True)\n",
    "    test_transaction[col].fillna(test_transaction[col].mode()[0], inplace=True)\n",
    "\n",
    "# 4. Fill Address Fields (addr1, addr2) with 'unknown' if missing\n",
    "train_transaction['addr1'].fillna('unknown', inplace=True)\n",
    "train_transaction['addr2'].fillna('unknown', inplace=True)\n",
    "\n",
    "test_transaction['addr1'].fillna('unknown', inplace=True)\n",
    "test_transaction['addr2'].fillna('unknown', inplace=True)\n",
    "\n",
    "# 5. Fill P_emaildomain with the mode (most common email domain)\n",
    "train_transaction['P_emaildomain'].fillna(train_transaction['P_emaildomain'].mode()[0], inplace=True)\n",
    "test_transaction['P_emaildomain'].fillna(test_transaction['P_emaildomain'].mode()[0], inplace=True)\n",
    "\n",
    "# Verify no nulls remain in these columns\n",
    "print(train_transaction[['D1', 'M1', 'card2', 'addr1', 'P_emaildomain']].isnull().sum())\n",
    "print(test_transaction[['D1', 'M1', 'card2', 'addr1', 'P_emaildomain']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pandas as pd\n",
    "# # from sklearn.impute import KNNImputer\n",
    "# # from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # # Select only the relevant columns for imputation\n",
    "# # train_addr = train_transaction[['addr1', 'addr2']]\n",
    "# # test_addr = test_transaction[['addr1', 'addr2']]\n",
    "\n",
    "# # # Standardize the columns to ensure KNN works effectively\n",
    "# # scaler = StandardScaler()\n",
    "# # train_addr_scaled = scaler.fit_transform(train_addr)\n",
    "# # test_addr_scaled = scaler.transform(test_addr)\n",
    "\n",
    "# # # Initialize the KNN imputer with a specified number of neighbors\n",
    "# # imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# # # Impute missing values\n",
    "# # train_addr_imputed = imputer.fit_transform(train_addr_scaled)\n",
    "# # test_addr_imputed = imputer.transform(test_addr_scaled)\n",
    "\n",
    "# # # Assign the imputed values back to the original columns\n",
    "# # train_transaction[['addr1', 'addr2']] = scaler.inverse_transform(train_addr_imputed)\n",
    "# # test_transaction[['addr1', 'addr2']] = scaler.inverse_transform(test_addr_imputed)\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# # Make copies of the data to avoid modifying the original directly\n",
    "# train_transaction = train_transaction.copy()\n",
    "# test_transaction = test_transaction.copy()\n",
    "\n",
    "# # Step 1: Label encode P_emaildomain to handle it as a numeric value\n",
    "# email_encoder = LabelEncoder()\n",
    "\n",
    "# # Fit the encoder on the combined dataset to handle any unique values in either dataset\n",
    "# combined_emaildomain = pd.concat([train_transaction['P_emaildomain'], test_transaction['P_emaildomain']])\n",
    "# email_encoder.fit(combined_emaildomain.dropna())\n",
    "\n",
    "# # Transform P_emaildomain in both train and test, setting NaN where originally missing\n",
    "# train_transaction['P_emaildomain_encoded'] = email_encoder.transform(\n",
    "#     train_transaction['P_emaildomain'].fillna('unknown')\n",
    "# )\n",
    "# test_transaction['P_emaildomain_encoded'] = email_encoder.transform(\n",
    "#     test_transaction['P_emaildomain'].fillna('unknown')\n",
    "# )\n",
    "\n",
    "# # Revert any imputed 'unknown' back to NaN to prepare for KNNImputer\n",
    "# train_transaction.loc[train_transaction['P_emaildomain'].isnull(), 'P_emaildomain_encoded'] = pd.NA\n",
    "# test_transaction.loc[test_transaction['P_emaildomain'].isnull(), 'P_emaildomain_encoded'] = pd.NA\n",
    "\n",
    "# # Step 2: Select columns for imputation and scale them\n",
    "# train_impute_data = train_transaction[['addr1', 'addr2', 'P_emaildomain_encoded']]\n",
    "# test_impute_data = test_transaction[['addr1', 'addr2', 'P_emaildomain_encoded']]\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# train_scaled = scaler.fit_transform(train_impute_data)\n",
    "# test_scaled = scaler.transform(test_impute_data)\n",
    "\n",
    "# # Step 3: Apply KNN Imputer\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# train_imputed = imputer.fit_transform(train_scaled)\n",
    "# test_imputed = imputer.transform(test_scaled)\n",
    "\n",
    "# # Step 4: Inverse transform the scaled data back to original scale\n",
    "# train_imputed_data = scaler.inverse_transform(train_imputed)\n",
    "# test_imputed_data = scaler.inverse_transform(test_imputed)\n",
    "\n",
    "# # Replace the original columns with imputed values\n",
    "# train_transaction[['addr1', 'addr2', 'P_emaildomain_encoded']] = train_imputed_data\n",
    "# test_transaction[['addr1', 'addr2', 'P_emaildomain_encoded']] = test_imputed_data\n",
    "\n",
    "# # Step 5: Decode the P_emaildomain from encoded values back to original categories\n",
    "# train_transaction['P_emaildomain'] = email_encoder.inverse_transform(\n",
    "#     train_transaction['P_emaildomain_encoded'].round().astype(int)\n",
    "# )\n",
    "# test_transaction['P_emaildomain'] = email_encoder.inverse_transform(\n",
    "#     test_transaction['P_emaildomain_encoded'].round().astype(int)\n",
    "# )\n",
    "\n",
    "# # Drop temporary encoded columns\n",
    "# train_transaction.drop(columns=['P_emaildomain_encoded'], inplace=True)\n",
    "# test_transaction.drop(columns=['P_emaildomain_encoded'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage = train_transaction.isnull().mean() * 100\n",
    "print(missing_percentage[missing_percentage > 0].index.tolist())\n",
    "\n",
    "missing_percentage = test_transaction.isnull().mean() * 100\n",
    "print(missing_percentage[missing_percentage > 0].index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['C1', 'C3', 'C5', 'C13']:\n",
    "    test_transaction[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Step 1: Select the columns for KNN imputation\n",
    "# columns_to_impute = ['C1', 'C3', 'C5', 'C13']\n",
    "# test_data_impute = test_transaction[columns_to_impute]\n",
    "\n",
    "# # Step 2: Standardize the selected columns for KNN compatibility\n",
    "# scaler = StandardScaler()\n",
    "# test_data_scaled = scaler.fit_transform(test_data_impute)\n",
    "\n",
    "# # Step 3: Apply KNN Imputer\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# test_data_imputed = imputer.fit_transform(test_data_scaled)\n",
    "\n",
    "# # Step 4: Inverse transform the scaled data back to original scale\n",
    "# test_data_imputed = scaler.inverse_transform(test_data_imputed)\n",
    "\n",
    "# # Replace the original columns with the imputed values\n",
    "# test_transaction[columns_to_impute] = test_data_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to M1 to M4 columns\n",
    "for col in ['M1', 'M2', 'M3', 'M4', \"M6\"]:\n",
    "    train_transaction[col] = label_encoder.fit_transform(train_transaction[col])\n",
    "    test_transaction[col] = label_encoder.fit_transform(test_transaction[col])\n",
    "    \n",
    "for col in ['card4', 'card6']:\n",
    "    encoder = LabelEncoder()\n",
    "    train_transaction[col] = encoder.fit_transform(train_transaction[col].astype(str))\n",
    "    test_transaction[col] = encoder.fit_transform(test_transaction[col].astype(str))\n",
    "\n",
    "# Replace 'unknown' with -1\n",
    "train_transaction[['addr1', 'addr2']] = train_transaction[['addr1', 'addr2']].replace('unknown', -1)\n",
    "test_transaction[['addr1', 'addr2']] = test_transaction[['addr1', 'addr2']].replace('unknown', -1)\n",
    "\n",
    "# Apply label encoding\n",
    "encoder = LabelEncoder()\n",
    "for col in ['addr1', 'addr2']:\n",
    "    train_transaction[col] = encoder.fit_transform(train_transaction[col].astype(str))\n",
    "    test_transaction[col] = encoder.fit_transform(test_transaction[col].astype(str))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the categories with their respective email domains\n",
    "categories = {\n",
    "    \"Free_Email_Providers\": {\n",
    "        \"gmail.com\", \"gmail\", \"outlook.com\", \"hotmail.com\", \"live.com\", \"live.com.mx\",\n",
    "        \"live.fr\", \"hotmail.es\", \"hotmail.fr\", \"hotmail.de\", \"hotmail.co.uk\", \n",
    "        \"outlook.es\", \"yahoo.com\", \"ymail.com\", \"rocketmail.com\", \"yahoo.fr\",\n",
    "        \"yahoo.de\", \"yahoo.es\", \"yahoo.co.uk\", \"yahoo.co.jp\", \"yahoo.com.mx\", \n",
    "        \"aol.com\", \"mail.com\"\n",
    "    },\n",
    "    \"ISP_Email_Domains\": {\n",
    "        \"verizon.net\", \"comcast.net\", \"optonline.net\", \"cox.net\", \"charter.net\", \n",
    "        \"sbcglobal.net\", \"bellsouth.net\", \"earthlink.net\", \"windstream.net\", \n",
    "        \"frontiernet.net\", \"frontier.com\", \"centurylink.net\", \"suddenlink.net\", \n",
    "        \"cableone.net\", \"twc.com\", \"cfl.rr.com\", \"sc.rr.com\", \"roadrunner.com\", \n",
    "        \"att.net\"\n",
    "    },\n",
    "    \"Legacy_Email\": {\n",
    "        \"hotmail.com\", \"msn.com\", \"aol.com\", \"juno.com\", \"prodigy.net.mx\", \n",
    "        \"embarqmail.com\", \"netzero.net\", \"netzero.com\", \"aim.com\", \"q.com\"\n",
    "    },\n",
    "    \"Business_Email_Domains\": {\n",
    "        \"servicios-ta.com\"\n",
    "    },\n",
    "    \"Private_Emails\": {\n",
    "        \"protonmail.com\", \"anonymous.com\"\n",
    "    },\n",
    "    \"Country_Specific\": {\n",
    "        \"gmx.de\", \"web.de\", \"yahoo.de\", \"hotmail.de\", \n",
    "        \"yahoo.fr\", \"hotmail.fr\", \"live.fr\", \n",
    "        \"yahoo.es\", \"hotmail.es\", \n",
    "        \"yahoo.co.uk\", \"hotmail.co.uk\", \n",
    "        \"prodigy.net.mx\", \"yahoo.com.mx\", \"live.com.mx\"\n",
    "    },\n",
    "    \"Apple_Domains\": {\n",
    "        \"me.com\", \"icloud.com\", \"mac.com\"\n",
    "    },\n",
    "    \"Miscellaneous_Providers\": {\n",
    "        \"ptd.net\", \"embarqmail.com\", \"rocketmail.com\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def classify_email_domain(domain):\n",
    "    \"\"\"Classify an email domain into one or more predefined categories.\"\"\"\n",
    "    matched_categories = set()\n",
    "\n",
    "    # Check which categories the domain belongs to\n",
    "    for category, domains in categories.items():\n",
    "        if domain in domains:\n",
    "            matched_categories.add(category)\n",
    "        if category == \"Country_Specific_Email_Providers\" and domain.endswith(\n",
    "                ('.de', '.fr', '.es', '.co.uk', '.mx')):\n",
    "            matched_categories.add(category)\n",
    "\n",
    "    if not matched_categories:\n",
    "        matched_categories.add(\"Unknown_or_Private_Domain\")\n",
    "    return matched_categories\n",
    "\n",
    "def classify_emails_in_df(df, email_column):\n",
    "    \"\"\"Add binary-encoded category columns to the DataFrame based on email domains.\"\"\"\n",
    "    # Extract the domain from the email column\n",
    "    df['domain'] = df[email_column].str.lower().fillna(\"Invalid_Email_Format\")\n",
    "\n",
    "    # Initialize category columns with 0\n",
    "    all_categories = list(categories.keys()) + [\"Unknown_or_Private_Domain\", \"Invalid_Email_Format\"]\n",
    "    for category in all_categories:\n",
    "        df[category] = 0\n",
    "\n",
    "    # Process each email domain\n",
    "    for index, row in df.iterrows():\n",
    "        domain = row['domain']\n",
    "        if domain == \"Invalid_Email_Format\":\n",
    "            matched_categories = {\"Invalid_Email_Format\"}\n",
    "        else:\n",
    "            matched_categories = classify_email_domain(domain)\n",
    "        \n",
    "        for category in matched_categories:\n",
    "            df.at[index, category] = 1\n",
    "\n",
    "    # Drop the temporary 'domain' column\n",
    "    df.drop('domain', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to classify email domains\n",
    "train_transaction = classify_emails_in_df(train_transaction, 'P_emaildomain')\n",
    "test_transaction = classify_emails_in_df(test_transaction, 'P_emaildomain')\n",
    "# Display the updated DataFrame\n",
    "print(train_transaction)\n",
    "print(\"--------\")\n",
    "print(test_transaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop manually\n",
    "manual_cols_to_drop = ['ProductCD', 'P_emaildomain', 'TransactionDT', 'day', 'TransactionAmt_bin', 'TransactionAmt'] # ID columns are not required\n",
    "\n",
    "# Drop the columns\n",
    "train_transaction.drop(columns=[col for col in manual_cols_to_drop if col in train_transaction], inplace=True)\n",
    "test_transaction.drop(columns=[col for col in manual_cols_to_drop if col in test_transaction], inplace=True)\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Remaining columns in Train Data:\")\n",
    "print(train_transaction.columns)\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Remaining columns in test Data:\")\n",
    "print(test_transaction.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns in train_data\n",
    "categorical_cols = train_transaction.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Display all categorical columns\n",
    "print(\"Categorical Columns in Train Transaction Data:\")\n",
    "print(list(categorical_cols))\n",
    "\n",
    "# Identify categorical columns in test_data\n",
    "categorical_cols = test_transaction.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Display all categorical columns\n",
    "print(\"Categorical Columns in Test Transaction Data:\")\n",
    "print(list(categorical_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute absolute correlation matrix\n",
    "corr_matrix = train_transaction.corr().abs()\n",
    "# Set a threshold for high correlation\n",
    "threshold = 0.75\n",
    "\n",
    "# Identify column pairs with high correlation\n",
    "high_corr_pairs = [\n",
    "    (col1, col2, corr_matrix.loc[col1, col2])\n",
    "    for col1 in corr_matrix.columns\n",
    "    for col2 in corr_matrix.columns\n",
    "    if col1 != col2 and corr_matrix.loc[col1, col2] > threshold\n",
    "]\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate a mask for the upper triangle to avoid redundancy\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', vmax=1.0, vmin=0.0, \n",
    "            center=0.5, annot=False, linewidths=0.5, square=True)\n",
    "\n",
    "plt.title('Correlation Matrix Heatmap', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "print(high_corr_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity Data\n",
    "\n",
    "null_counts = train_identity.isna().sum()\n",
    "null_percentage = (null_counts / len(train_identity)) * 100\n",
    "\n",
    "# Create a DataFrame summarizing the null values\n",
    "null_summary = pd.DataFrame({'Null Values': null_counts, 'Percentage': null_percentage})\n",
    "\n",
    "# Display the columns sorted by the percentage of null values (descending)\n",
    "print(\"Null Values Summary:\")\n",
    "display(null_summary.sort_values(by='Percentage', ascending=False))\n",
    "\n",
    "# Identity Data\n",
    "\n",
    "null_counts = test_identity.isna().sum()\n",
    "null_percentage = (null_counts / len(test_identity)) * 100\n",
    "\n",
    "# Create a DataFrame summarizing the null values\n",
    "null_summary = pd.DataFrame({'Null Values': null_counts, 'Percentage': null_percentage})\n",
    "\n",
    "# Display the columns sorted by the percentage of null values (descending)\n",
    "print(\"Null Values Summary:\")\n",
    "display(null_summary.sort_values(by='Percentage', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with the most frequent value\n",
    "most_frequent_type = train_identity['DeviceType'].mode()[0]\n",
    "train_identity['DeviceType'].fillna(most_frequent_type, inplace=True)\n",
    "\n",
    "# Get mode for DeviceInfo separately for mobile and desktop\n",
    "mobile_mode = train_identity[train_identity['DeviceType'] == 'mobile']['DeviceInfo'].mode()[0]\n",
    "desktop_mode = train_identity[train_identity['DeviceType'] == 'desktop']['DeviceInfo'].mode()[0]\n",
    "\n",
    "# Function to fill missing DeviceInfo based on DeviceType\n",
    "def fill_device_info(row):\n",
    "    if pd.isnull(row['DeviceInfo']):\n",
    "        if row['DeviceType'] == 'mobile':\n",
    "            return mobile_mode\n",
    "        elif row['DeviceType'] == 'desktop':\n",
    "            return desktop_mode\n",
    "        else:\n",
    "            return 'Unknown'  # Handle cases where DeviceType is also missing\n",
    "    return row['DeviceInfo']\n",
    "\n",
    "# Apply the function to fill missing DeviceInfo values\n",
    "train_identity['DeviceInfo'] = train_identity.apply(fill_device_info, axis=1)\n",
    "\n",
    "# Verify the result\n",
    "print(train_identity['DeviceInfo'].isnull().sum())  # Should be 0 if all missing values are filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with the most frequent value\n",
    "most_frequent_type = test_identity['DeviceType'].mode()[0]\n",
    "test_identity['DeviceType'].fillna(most_frequent_type, inplace=True)\n",
    "\n",
    "# Get mode for DeviceInfo separately for mobile and desktop\n",
    "mobile_mode = test_identity[test_identity['DeviceType'] == 'mobile']['DeviceInfo'].mode()[0]\n",
    "desktop_mode = test_identity[test_identity['DeviceType'] == 'desktop']['DeviceInfo'].mode()[0]\n",
    "\n",
    "# Function to fill missing DeviceInfo based on DeviceType\n",
    "def fill_device_info(row):\n",
    "    if pd.isnull(row['DeviceInfo']):\n",
    "        if row['DeviceType'] == 'mobile':\n",
    "            return mobile_mode\n",
    "        elif row['DeviceType'] == 'desktop':\n",
    "            return desktop_mode\n",
    "        else:\n",
    "            return 'Unknown'  # Handle cases where DeviceType is also missing\n",
    "    return row['DeviceInfo']\n",
    "\n",
    "# Apply the function to fill missing DeviceInfo values\n",
    "test_identity['DeviceInfo'] = test_identity.apply(fill_device_info, axis=1)\n",
    "\n",
    "# Verify the result\n",
    "print(test_identity['DeviceInfo'].isnull().sum())  # Should be 0 if all missing values are filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Function to Extract Device Brand or Name\n",
    "def extract_device_name(device_info):\n",
    "    # Convert to string and lower case for uniformity\n",
    "    device_info = str(device_info).lower() \n",
    "    sony_models = ['e6853', 'sov33', 'd5803', 'g3313', 'f3213', 'f3113', 'g3223', 'f5121','f3313', 'e6603', \"e2306\", \"f8331\", \"f3111\", \"d6603\", \"f5321\", \"e5506\", \"e5823\", \"g3123\", \"c6906\", \"e5803\", \"e2104\", \"e6553\", \"d5306\", \"d5316\", \"e6810\", \"e5306\", \"g8141\", \"g3423\", \"f3111 build/33.3.a.1.115\", \"e6683\", \"e6633\", \"e6653\", \"c6903\", \"lt22i\", \"lt30p\", \"sgp521\", \"sgp511\", \"sgp611\", \"sgp621\", \"f8131\", \"f5122\", \"f8332\", \"e2006\", \"d6708\", \"d6503\", \"e6683\", \"e5606\", \"f5121\", \"e5306 build/27.3.a.0.129\", \"e5306 build/27.3.a.0.165\", \"g3123 build/40.0.a.6.175\", \"g3123 build/40.0.a.6.189\", \"g8141 build/47.1.a.5.51\", \"d5316 build/19.4.a.0.182\", \"d5306 build/19.4.a.0.182\", \"e2104 build/24.0.a.5.14\", \"d2306 build/18.6.a.0.182\", \"e5823 build/32.4.a.1.54\", \"e6653 build/32.4.a.1.54\", \"c6603\", \"e5803\", \"e5823\", \"e6553\", \"g8142\", \"g8341\", \"f8331 build/41.2.a.7.76\", \"f5321 build/34.3.a.0.238\", \"g3123\", \"e2303\", \"d6603 build/23.5.a.1.291\", \"sgp621 build/23.5.a.1.291\", \"e2306\", \"f8331\", \"f3111\", \"d6603\", \"f5321\", \"e5506\", \"e5823\", \"g3123\", \"c6906\", \"e5803\", \"e2104\", \"e6553\", \"d5306\", \"d5316\", \"e6810\", \"e5306\", \"g8141\", \"g3423\", \"f3111 build/33.3.a.1.115\", \"e6683\", \"e6633\", \"e6653\", \"c6903\", \"lt22i\", \"lt30p\", \"sgp521\", \"sgp511\", \"sgp611\", \"sgp621\", \"f8131\", \"f5122\", \"f8332\", \"e2006\", \"d6708\", \"d6503\", \"e5606\", \"f5121\", \"e5306 build/27.3.a.0.129\", \"e5306 build/27.3.a.0.165\", \"g3123 build/40.0.a.6.175\", \"g3123 build/40.0.a.6.189\", \"g8141 build/47.1.a.5.51\", \"d5316 build/19.4.a.0.182\", \"d5306 build/19.4.a.0.182\", \"e2104 build/24.0.a.5.14\", \"d2306 build/18.6.a.0.182\", \"e5823 build/32.4.a.1.54\", \"e6653 build/32.4.a.1.54\", \"g8142\", \"g8341\", \"f8331 build/41.2.a.7.76\", \"f5321 build/34.3.a.0.238\", \"e2303\", \"d6603 build/23.5.a.1.291\", \"sgp621 build/23.5.a.1.291\"]\n",
    "    alacatel_models = ['6045i','5095i' '5080a', '5010g', '8050g', '5025g', '5015a', '5054s','5056a', '5012g', \"4047g\", \"4013m\", \"5010s\", \"5085b\", \"5049w\", \"9008a\", \"9003a\", \"4047a\", \"5051a\", \"5045i\", \"6039a\", \"7048a\", \"5056n\", \"5042a\", \"5017a\", \"4034g\", \"4034e\", \"4009f\", \"4027a\", \"5057m\", \"6037b\"]\n",
    "    amz_fire_models = [\"kftuwi\", \"kfsnwi\", \"kfrawi\", \"kfrapwi\", \"kfquwi\", \"kftrwi\", \"kftrpwi\", \"kfonwi\", \"kfmawi\", \"kfmuwi\", \"kfkawi\", \"kfsuwi\", \"kfdowi\", \"kfauwi\", \"kfgiwi\", \"kftbwi\", \"kfmewi\", \"kffowi\", \"kfsawa\", \"kfsawi\", \"kfaswi\", \"kfarwi\", \"kfthwa\", \"kfthwi\", \"kfapwa\", \"kfapwi\", \"kfsowi\", \"kfot\", \"kftt\", \"kfjwa\", \"kfjwi\", \"kindle fire\"];\n",
    "    # Nokia\n",
    "    nokiaModels = [\"ta-1039\", \"ta-1028\", \"ta-1044\", \"ta-1025\", \"ta-1027\", \"ta-1003\", \"ta-1004\", \"ta-1032\", \"ta-1020\", \"ta-1028 build/nmf26o\", \"ta-1027 build/opr1.170623.026\", \"ta-1025 build/opr1.170623.026\", \"ta-1044 build/opr1.170623.026\", \"ta-1038\", \"ta-1038 build/o00623\", \"ta-1028 build/o00623\", \"ta-1027 build/n2g47h\"]\n",
    "    # BLU Products\n",
    "    bluModels = [\"blu\", \"blu studio c 5+5\", \"blu energy x plus\", \"blu life xl\", \"studio\", \"studio_g_hd\", \"blu energy x 2 build/e050l\"]\n",
    "    # ZTE\n",
    "    zteModels = [\"z981\", \"z982\", \"z956\", \"z813\", \"z798bl\", \"z832\", \"z799vl\", \"z837vl\", \"z831\", \"z963vl\", \"z965\", \"z962bl\", \"z557bl\", \"z812\", \"z839\", \"z836bl\", \"z955a\", \"z970\", \"z959 build/lmy47v\", \"z833\"]\n",
    "    \n",
    "\n",
    "    # Huawei/Honor\n",
    "    huaweiModels = [\"hi6210sft\", \"bln-l21\", \"bln-l24\", \"dli-l22\", \"nem-l51\", \"sla-l23\", \"sla-l22\", \"sla-l02\", \"bnd-l21\", \"bnd-l34\", \"vtr-l29\", \"was-lx2j\", \"pe-tl10\", \"plk-l01\", \"cam-l23\", \"mya-l23\", \"mya-l11\", \"duk-al20\", \"kiw-l24\", \"g620s-l03\", \"g630-u251\", \"g527-u081\"]\n",
    "\n",
    "    # OnePlus\n",
    "    oneplusModels = [\"OnePlus\", \"one a2005\", \"one a2003\", \"a0001\", \"a0001 build/mhc19q\", \"2ps64 build/nrd90m\"]\n",
    "\n",
    "    # LG\n",
    "    lgModels = [\"k88\", \"lm-x210(g\", \"rs988\", \"h1611\", \"ls5\", \"vk700\", \"vk810\", \"vk815\", \"k90u\", \"p5006a\", \"p5046a\"]\n",
    "\n",
    "    # Oppo\n",
    "    oppoModels = [\"Oppo/Vivo\", \"a37f\", \"a96\", \"f1f\", \"cph1701\", \"cph1607\", \"cph1723\"]\n",
    "\n",
    "    # HTC\n",
    "    htcModels = [\"2ps64\", \"0pm92\", \"2pzc5\", \"0paj5\", \"0pja2\", \"2pq93\", \"2pyb2\"]\n",
    "\n",
    "    # BlackBerry\n",
    "    blackberryModels = [\"stv100-4\", \"stv100-1\", \"stv100-2\", \"stv100-3\", \"bbb100-1\", \"bbb100-2\", \"bbb100-3\", \"bba100-1\", \"bba100-2\"]\n",
    "\n",
    "    # Asus\n",
    "    asusModels = [\"p008\", \"p00c\", \"p00a\", \"me173x\", \"me301t\", \"p027\", \"t1\", \"t08\", \"p01m\"]\n",
    "\n",
    "    # BQ\n",
    "    bqModels = [\"aquaris\", \"aquaris v\", \"aquaris x\", \"aquaris x5 plus\", \"aquaris u plus\", \"aquaris_a4.5\", \"fractal\"]\n",
    "\n",
    "    # LeEco\n",
    "    leecoModels = [\"le x520\", \"le x829\", \"lex829\"]\n",
    "\n",
    "    # ZUK\n",
    "    zukModels = [\"z2\"]\n",
    "\n",
    "    # Kyocera\n",
    "    kyoceraModels = [\"e6810\", \"kyocera-c6742a\", \"e6790tm\", \"c6743\"]\n",
    "\n",
    "    # Cat Phones\n",
    "    catModels = [\"s60\"]\n",
    "\n",
    "    # TP-Link\n",
    "    tplinkModels = [\"neffos c5\", \"neffos x1 max build/nrd90m\"]\n",
    "\n",
    "    # Motorola\n",
    "    motorolaModels = [\"xt1650\", \"xt1575\", \"xt1254\", \"xt1585\"]\n",
    "\n",
    "    # RCA\n",
    "    rcaModels = [\"rct6203w46\", \"rct6223w87\", \"rct6303w87m7\", \"rct6s03w12\", \"rct6k03w13\", \"rct6513w87\", \"rct6773w22b\"]\n",
    "\n",
    "    # Lava\n",
    "    lavaModels = [\"iris 870\", \"iris 820\", \"lava_a3\", \"iris702\", \"iris50\", \"iris80\"]\n",
    "\n",
    "    # Wiko\n",
    "    wikoModels = [\"pulp 4g\", \"highway\", \"fever\", \"u feel lite build/mra58k\"]\n",
    "\n",
    "    # Crosscall\n",
    "    crosscallModels = [\"trekker-x3\", \"trekker-m1\"]\n",
    "\n",
    "    # Vivo\n",
    "    vivoModels = [\"Oppo/Vivo\", \"v502015\"]\n",
    "\n",
    "    # Dell\n",
    "    dellModels = [\"venue\"]\n",
    "\n",
    "    # Lenovo\n",
    "    lenovoModels = [\"mot-a6020l37\", \"ideatab\", \"ideataba1000-f\", \"ideataba2109a\", \"a1-850\", \"a3-a20\", \"b1-790\", \"b1-750\", \"b1-810\", \"tab2a7-10f\", \"yoga\", \"ple-701l\"]\n",
    "\n",
    "    # Blackview\n",
    "    blackviewModels = [\"bv7000\", \"bv6000\", \"bv8000pro\"]\n",
    "\n",
    "    # Verykool\n",
    "    verykoolModels = [\"verykool\", \"s471\"]\n",
    "\n",
    "    # Archos\n",
    "    archosModels = [\"archos\"]\n",
    "\n",
    "    # Gigaset\n",
    "    gigasetModels = [\"gigaset\"]\n",
    "\n",
    "    # Coolpad\n",
    "    coolpadModels = [\"coolpad\"]\n",
    "\n",
    "    # Leagoo\n",
    "    leagooModels = [\"leagoo kiicaa mix\"]\n",
    "\n",
    "    # Nvidia\n",
    "    nvidiaModels = [\"shield\"]  # If present in data\n",
    "\n",
    "    # Ramos\n",
    "    ramosModels = [\"ramos\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define patterns to extract meaningful device names\n",
    "    if 'samsung' in device_info or 'sm-' in device_info or 'gt-' in device_info or 'sgh-' in device_info or 'sch-' in device_info or 'sgh-' in device_info:\n",
    "        return 'Samsung'\n",
    "    elif 'redmi' in device_info or 'mi' in device_info:\n",
    "        return 'Xiaomi'\n",
    "    elif 'moto' in device_info or 'motorola' in device_info:\n",
    "        return 'Motorola'\n",
    "    elif 'pixel' in device_info:\n",
    "        return 'Google Pixel'\n",
    "    elif 'htc' in device_info:\n",
    "        return 'HTC'\n",
    "    elif 'lg' in device_info:\n",
    "        return 'LG'\n",
    "    elif 'huawei' in device_info:\n",
    "        return 'Huawei'\n",
    "    elif 'iphone' in device_info or 'ios' in device_info:\n",
    "        return 'Apple'\n",
    "    elif 'nexus' in device_info:\n",
    "        return 'Google Nexus'\n",
    "    elif 'lenovo' in device_info:\n",
    "        return 'Lenovo'\n",
    "    elif 'oneplus' in device_info:\n",
    "        return 'OnePlus'\n",
    "    elif 'windows' in device_info:\n",
    "        return 'Windows'\n",
    "    elif 'alcatel' in device_info or 'one touch' in device_info or any(model in device_info for model in alacatel_models) :\n",
    "        return 'Alcatel'\n",
    "    elif 'sony' in device_info or 'xperia' in device_info or any(model in device_info for model in sony_models) :\n",
    "        return 'Sony'\n",
    "    elif 'nokia' in device_info or any(model in device_info for model in nokiaModels):\n",
    "        return 'Nokia'\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    elif any(model.lower() in device_info for model in bluModels):\n",
    "        return 'Blu'\n",
    "    elif any(model.lower() in device_info for model in zteModels):\n",
    "        return 'ZTE'\n",
    "    elif any(model.lower() in device_info for model in huaweiModels):\n",
    "        return 'Huawei'\n",
    "    elif any(model.lower() in device_info for model in oneplusModels):\n",
    "        return 'OnePlus'\n",
    "    elif any(model.lower() in device_info for model in lgModels):\n",
    "        return 'LG'\n",
    "    elif any(model.lower() in device_info for model in oppoModels):\n",
    "        return 'Oppo'\n",
    "    if any(model.lower() in device_info for model in htcModels):\n",
    "        return 'HTC'\n",
    "    elif any(model.lower() in device_info for model in blackberryModels):\n",
    "        return 'BlackBerry'\n",
    "    elif any(model.lower() in device_info for model in asusModels):\n",
    "        return 'Asus'\n",
    "    elif any(model.lower() in device_info for model in bqModels):\n",
    "        return 'BQ'\n",
    "    elif any(model.lower() in device_info for model in leecoModels):\n",
    "        return 'LeEco'\n",
    "    elif any(model.lower() in device_info for model in zukModels):\n",
    "        return 'ZUK'\n",
    "    elif any(model.lower() in device_info for model in kyoceraModels):\n",
    "        return 'Kyocera'\n",
    "    elif any(model.lower() in device_info for model in catModels):\n",
    "        return 'CAT'\n",
    "    elif any(model.lower() in device_info for model in tplinkModels):\n",
    "        return 'TP-Link'\n",
    "    elif any(model.lower() in device_info for model in motorolaModels):\n",
    "        return 'Motorola'\n",
    "    elif any(model.lower() in device_info for model in rcaModels):\n",
    "        return 'RCA'\n",
    "    elif any(model.lower() in device_info for model in lavaModels):\n",
    "        return 'Lava'\n",
    "    elif any(model.lower() in device_info for model in wikoModels):\n",
    "        return 'Wiko'\n",
    "    elif any(model.lower() in device_info for model in crosscallModels):\n",
    "        return 'Crosscall'\n",
    "    elif any(model.lower() in device_info for model in vivoModels):\n",
    "        return 'Vivo'\n",
    "    elif any(model.lower() in device_info for model in dellModels):\n",
    "        return 'Dell'\n",
    "    elif any(model.lower() in device_info for model in lenovoModels):\n",
    "        return 'Lenovo'\n",
    "    elif any(model.lower() in device_info for model in blackviewModels):\n",
    "        return 'Blackview'\n",
    "    elif any(model.lower() in device_info for model in verykoolModels):\n",
    "        return 'Verykool'\n",
    "    elif any(model.lower() in device_info for model in archosModels):\n",
    "        return 'Archos'\n",
    "    elif any(model.lower() in device_info for model in gigasetModels):\n",
    "        return 'Gigaset'\n",
    "    elif any(model.lower() in device_info for model in coolpadModels):\n",
    "        return 'Coolpad'\n",
    "    elif any(model.lower() in device_info for model in leagooModels):\n",
    "        return 'Leagoo'\n",
    "    elif any(model.lower() in device_info for model in nvidiaModels):\n",
    "        return 'Nvidia'\n",
    "    elif any(model.lower() in device_info for model in ramosModels):\n",
    "        return 'Ramos'\n",
    "    ###\n",
    "    \n",
    "    elif 'oppo' in device_info or 'vivo' in device_info:\n",
    "        return 'Oppo/Vivo'\n",
    "    elif 'asus' in device_info:\n",
    "        return 'Asus'\n",
    "    elif 'zte' in device_info:\n",
    "        return 'ZTE'\n",
    "    elif 'tablet' in device_info:\n",
    "        return 'Tablet'\n",
    "    elif 'linux' in device_info:\n",
    "        return 'Linux'\n",
    "    elif 'hisense' in device_info:\n",
    "        return 'Hisense'\n",
    "    elif 'blade' in device_info or 'z983 ' in device_info:\n",
    "        return 'ZTE'\n",
    "    elif 'ilium ' in device_info:\n",
    "        return 'Lanix Ilium'\n",
    "    elif 'm4 ' in device_info:\n",
    "        return 'M4tel'\n",
    "    elif device_info.startswith('xt'):\n",
    "        if 'sony' in device_info.lower():\n",
    "            return 'Sony'\n",
    "        return 'Motorola'\n",
    "    elif any(model in device_info for model in amz_fire_models):\n",
    "        return 'Amazon Fire'\n",
    "    elif device_info.startswith('vs'):\n",
    "        return 'LG'\n",
    "    elif device_info.startswith('verykools'):\n",
    "        return 'Verykool'\n",
    "    else:\n",
    "        # Default to 'Other' if no meaningful match is found\n",
    "        return 'Other'\n",
    "\n",
    "# Step 2: Apply the Function to the 'DeviceInfo' Column\n",
    "train_identity['device_name'] = train_identity['DeviceInfo'].apply(extract_device_name)\n",
    "\n",
    "test_identity['device_name'] = test_identity['DeviceInfo'].apply(extract_device_name)\n",
    "# Verify the Results\n",
    "# print(train_identity[['DeviceInfo', 'device_name']].head(20))\n",
    "\n",
    "device_name_counts = train_identity['device_name'].value_counts()\n",
    "\n",
    "print(len(device_name_counts))\n",
    "print(device_name_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity Data\n",
    "\n",
    "null_counts = train_identity.isna().sum()\n",
    "null_percentage = (null_counts / len(train_identity)) * 100\n",
    "\n",
    "# Create a DataFrame summarizing the null values\n",
    "null_summary = pd.DataFrame({'Null Values': null_counts, 'Percentage': null_percentage})\n",
    "\n",
    "# Display the columns sorted by the percentage of null values (descending)\n",
    "print(\"Null Values Summary:\")\n",
    "display(null_summary.sort_values(by='Percentage', ascending=False))\n",
    "\n",
    "# Identify columns with >95% missing values\n",
    "columns_to_drop = null_summary[null_summary['Percentage'] > 95].index\n",
    "\n",
    "# Drop these columns\n",
    "train_identity.drop(columns=[col for col in columns_to_drop if col in train_identity], inplace=True)\n",
    "test_identity.drop(columns=[col for col in columns_to_drop if col in test_identity], inplace=True)\n",
    "\n",
    "print(f\"Dropped columns: {list(columns_to_drop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_device_types = train_identity['DeviceType'].unique()\n",
    "\n",
    "# Display the distinct values\n",
    "print(\"Distinct values in 'DeviceType':\")\n",
    "print(distinct_device_types)\n",
    "\n",
    "device_type_mapping = {'mobile': 1, 'desktop': 0, 'missing': -1}\n",
    "train_identity['device_type_numeric'] = train_identity['DeviceType'].map(device_type_mapping)\n",
    "test_identity['device_type_numeric'] = test_identity['DeviceType'].map(device_type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Extract OS Name\n",
    "def extract_os_name(os):\n",
    "    # Match the base OS (e.g., 'Android', 'iOS', 'Windows', 'Mac OS X', 'Linux')\n",
    "    match = re.match(r'(Android|iOS|Mac OS X|Windows|Linux|other|func)', str(os))\n",
    "    return match.group(0) if match else 'Missing'\n",
    "\n",
    "# Step 2: Extract OS Version\n",
    "def extract_os_version(os):\n",
    "    # Extract version numbers (e.g., '7.0', '10_11_6', '11.2.1')\n",
    "    match = re.search(r'(\\d+[\\._]?\\d*[\\._]?\\d*)', str(os))\n",
    "    return match.group(0) if match else 'Missing'\n",
    "\n",
    "# Apply the functions to the 'id_30' column\n",
    "train_identity['os_name'] = train_identity['id_30'].apply(extract_os_name)\n",
    "train_identity['os_version'] = train_identity['id_30'].apply(extract_os_version)\n",
    "\n",
    "test_identity['os_name'] = test_identity['id-30'].apply(extract_os_name)\n",
    "test_identity['os_version'] = test_identity['id-30'].apply(extract_os_version)\n",
    "\n",
    "# Verify the transformed columns\n",
    "print(train_identity[['id_30', 'os_name', 'os_version']].head(20))\n",
    "\n",
    "# Step 1: Extract Browser Name\n",
    "def extract_browser_name(browser):\n",
    "    # Match common browser names or keywords\n",
    "    match = re.match(\n",
    "        r'(chrome|safari|mobile safari|firefox|edge|opera|samsung browser|ie|android browser|'\n",
    "        r'google|puffin|waterfox|cyberfox|maxthon|palemoon|iron|silk|facebook|'\n",
    "        r'chromium|seamonkey|aol|generic)', \n",
    "        str(browser).lower()\n",
    "    )\n",
    "    return match.group(0) if match else 'Missing'\n",
    "\n",
    "# Step 2: Extract Browser Version\n",
    "def extract_browser_version(browser):\n",
    "    # Extract version numbers (e.g., '62.0', '11.0')\n",
    "    match = re.search(r'(\\d+[\\._]?\\d*)', str(browser))\n",
    "    return match.group(0) if match else 'Missing'\n",
    "\n",
    "# Apply the functions to the 'id_31' column\n",
    "train_identity['browser_name'] = train_identity['id_31'].apply(extract_browser_name)\n",
    "train_identity['browser_version'] = train_identity['id_31'].apply(extract_browser_version)\n",
    "\n",
    "test_identity['browser_name'] = test_identity['id-31'].apply(extract_browser_name)\n",
    "test_identity['browser_version'] = test_identity['id-31'].apply(extract_browser_version)\n",
    "\n",
    "# Verify the transformation\n",
    "print(train_identity[['id_31', 'browser_name', 'browser_version']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "num_cols = train_identity.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = train_identity.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute numerical columns with median\n",
    "for col in num_cols:\n",
    "    if train_identity[col].isnull().sum() > 0:\n",
    "        train_identity[col].fillna(-1, inplace=True)\n",
    "        test_identity[col.replace(\"_\", \"-\")].fillna(-1, inplace=True)\n",
    "\n",
    "for col in cat_cols:\n",
    "    if train_identity[col].isnull().sum() > 0:\n",
    "        train_identity[col].fillna('Missing', inplace=True)\n",
    "        test_identity[col.replace(\"_\", \"-\")].fillna('Missing', inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop manually\n",
    "manual_cols_to_drop = ['id_30', 'id_31', 'id_33', 'DeviceType', 'DeviceInfo'] # ID columns are not required\n",
    "\n",
    "# # Drop the columns\n",
    "train_identity.drop(columns=[col for col in manual_cols_to_drop if col in train_identity], inplace=True)\n",
    "test_identity.drop(columns=[col.replace(\"_\", \"-\") for col in manual_cols_to_drop if col.replace(\"_\", \"-\") in test_identity], inplace=True)\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Remaining columns in Train Data:\")\n",
    "print(train_identity.columns)\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Remaining columns in Train Data:\")\n",
    "print(test_identity.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all '-' with '_' in column names\n",
    "test_identity.columns = test_identity.columns.str.replace('-', '_', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Select all categorical columns\n",
    "cat_cols = train_identity.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    train_identity[col] = le.fit_transform(train_identity[col])\n",
    "    test_identity[col] = le.fit_transform(test_identity[col])\n",
    "\n",
    "# Verify the transformed data\n",
    "print(train_identity[cat_cols].head())\n",
    "print(test_identity[cat_cols].head())\n",
    "\n",
    "\n",
    "# def print_unique_values(df, columns):\n",
    "#     for col in columns:\n",
    "#         unique_values = df[col].unique()\n",
    "#         print(f\"Column: {col}, Unique Values: {unique_values}\\n\")\n",
    "\n",
    "\n",
    "# print(train_identity, [cat_cols[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_identity.corr().abs()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt='.2f', \n",
    "            vmax=1.0, vmin=0.0, linewidths=0.5, square=True)\n",
    "\n",
    "plt.title('Correlation Matrix for Train Identity Data', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "threshold = 0.85\n",
    "\n",
    "# Identify pairs of columns with high correlation above the threshold\n",
    "high_corr_pairs = np.where(np.abs(corr_matrix) > threshold)\n",
    "\n",
    "# Filter out self-correlations (same column correlations)\n",
    "high_corr_pairs = [\n",
    "    (corr_matrix.index[i], corr_matrix.columns[j], corr_matrix.iloc[i, j])\n",
    "    for i, j in zip(*high_corr_pairs)\n",
    "    if i != j and i < j  # Avoid duplicate pairs and self-correlation\n",
    "]\n",
    "\n",
    "# Print high-correlation column pairs\n",
    "print(\"Highly Correlated Columns:\")\n",
    "for col1, col2, corr_value in high_corr_pairs:\n",
    "    print(f\"{col1} - {col2} : {corr_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['id_32', 'id_34', 'id_35', 'id_28', 'id_29']\n",
    "\n",
    "# Drop the columns if they exist in the DataFrame\n",
    "train_identity.drop(columns=[col for col in columns_to_drop if col in train_identity.columns], inplace=True)\n",
    "test_identity.drop(columns=[col for col in columns_to_drop if col in test_identity.columns], inplace=True)\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(train_identity.head())\n",
    "print(test_identity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "merged_test_data = test_transaction.merge(test_identity, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_test_data.columns.tolist())\n",
    "mdc = merged_data.columns.tolist()\n",
    "mdc.remove(\"isFraud\")\n",
    "print(mdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns of merged_test_data\n",
    "print(merged_test_data.columns.tolist())\n",
    "\n",
    "# Convert columns to lists for easier manipulation\n",
    "mdc = merged_data.columns.tolist()\n",
    "\n",
    "# Remove \"isFraud\" from mdc if it exists\n",
    "if \"isFraud\" in mdc:\n",
    "    mdc.remove(\"isFraud\")\n",
    "\n",
    "print(mdc)\n",
    "\n",
    "# Filter merged_test_data to keep only columns present in mdc\n",
    "merged_test_data = merged_test_data[merged_test_data.columns.intersection(mdc)]\n",
    "\n",
    "# Print the columns of the filtered DataFrame\n",
    "print(merged_test_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = merged_data.isnull().sum()\n",
    "print(null_counts[null_counts > 0])\n",
    "\n",
    "merged_data.fillna(-1, inplace=True)\n",
    "\n",
    "null_counts = merged_test_data.isnull().sum()\n",
    "print(null_counts[null_counts > 0])\n",
    "\n",
    "merged_test_data.fillna(-1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data.drop(['isFraud', 'TransactionID'], axis=1)\n",
    "y = merged_data['isFraud']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_data = pd.read_csv('/Users/manideepreddyaliminati/Documents/ASUCourses/CSE572/notebook/test_transaction.csv')\n",
    "\n",
    "# Merge back the 'TransactionID' column\n",
    "merged_test_data['TransactionID'] = original_test_data['TransactionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Step 1: Split train data into features and target\n",
    "X = merged_data.drop(['isFraud', 'TransactionID'], axis=1)  # Features\n",
    "y = merged_data['isFraud']  # Target variable\n",
    "\n",
    "# Step 2: Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original dataset shape: {X_train.shape}, {y_train.value_counts()}\")\n",
    "print(f\"Resampled dataset shape: {X_train_resampled.shape}, {y_train_resampled.value_counts()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp tuning randomforest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 10, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(\n",
    "    rf, param_grid, cv=5, scoring=\"roc_auc\", n_jobs=-1, verbose=2\n",
    ")\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_train, y_train, test_size=0.9, stratify=y_train, random_state=42\n",
    ")\n",
    "grid_search.fit(X_small, y_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp tuning randomforest\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score(reduced dataset):\", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Step 4: Make predictions on the validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "print(\"Random Forest after hp tuning...\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_val, y_prob):.4f}\")\n",
    "\n",
    "# Step 6: Run predictions on the test set\n",
    "X_test = merged_test_data.loc[:, merged_test_data.columns != \"TransactionID\"]\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 7: Prepare submission DataFrame\n",
    "submission_rfc = pd.DataFrame(\n",
    "    {\n",
    "        \"TransactionID\": merged_test_data[\n",
    "            \"TransactionID\"\n",
    "        ],  # Use the 'TransactionID' column\n",
    "        \"isFraud\": test_prob,  # Use the predictions\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 8: Save predictions to a CSV file\n",
    "submission_rfc.to_csv(\"submission_rfc_2.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to submission_rfc_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp tuning for xgboost classifier\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_space = {\n",
    "    \"learning_rate\": (0.01, 0.3),\n",
    "    \"max_depth\": (3, 10),\n",
    "    \"min_child_weight\": (1, 20),\n",
    "    \"subsample\": (0.5, 1.0),\n",
    "    \"colsample_bytree\": (0.5, 1.0),\n",
    "    \"n_estimators\": (100, 1000),\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "bayes_search = BayesSearchCV(\n",
    "    xgb_model, param_space, n_iter=50, cv=5, scoring=\"roc_auc\", n_jobs=-1, verbose=2\n",
    ")\n",
    "\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_train, y_train, test_size=0.9, stratify=y_train, random_state=42\n",
    ")\n",
    "bayes_search.fit(X_small, y_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp tuning for xgboost classifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Best parameters found:\", bayes_search.best_params_)\n",
    "print(\"Best score achieved:(reduced data)\", bayes_search.best_score_)\n",
    "\n",
    "best_model = bayes_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions on the validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "print(\"XGBoost\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f\"AUC-ROC Score: {roc_auc_score(y_val, y_prob):.4f}\")\n",
    "\n",
    "# Step 6: Run predictions on the test set\n",
    "X_test = merged_test_data.loc[:, merged_test_data.columns != \"TransactionID\"]\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 7: Prepare submission DataFrame\n",
    "submission_xgb = pd.DataFrame(\n",
    "    {\n",
    "        \"TransactionID\": merged_test_data[\n",
    "            \"TransactionID\"\n",
    "        ],  # Use the 'TransactionID' column\n",
    "        \"isFraud\": test_prob,  # Use the predictions\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 8: Save predictions to a CSV file\n",
    "submission_xgb.to_csv(\"submission_xgb_ft_2.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to submission_xgb_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import optuna\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Create timestamped folder\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_folder = f\"bayesian_optimization_results_{timestamp}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "log_file = os.path.join(output_folder, \"log.txt\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    filemode=\"a\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"newton-cholesky\", \"sag\", \"saga\"])\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l2\", \"none\", \"l1\"])\n",
    "    C = trial.suggest_loguniform(\"C\", 1e-3, 1e3)\n",
    "    tol = trial.suggest_loguniform(\"tol\", 1e-5, 1e-1)\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "    l1_ratio = None  # ElasticNet compatibility not handled here\n",
    "\n",
    "    # Compatibility checks for solver and penalty\n",
    "    if solver == \"newton-cholesky\" and penalty not in [\"l2\", \"none\"]:\n",
    "        logger.warning(\n",
    "            f\"Trial {trial.number}: Invalid combination (solver={solver}, penalty={penalty}). Pruning trial.\"\n",
    "        )\n",
    "        raise optuna.exceptions.TrialPruned()  # Invalid combination\n",
    "    if solver == \"sag\" and penalty != \"l2\":\n",
    "        logger.warning(\n",
    "            f\"Trial {trial.number}: Invalid combination (solver={solver}, penalty={penalty}). Pruning trial.\"\n",
    "        )\n",
    "        raise optuna.exceptions.TrialPruned()  # Invalid combination\n",
    "\n",
    "    logger.info(f\"Trial {trial.number}: Starting with parameters: {trial.params}\")\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_aucs = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train_resampled, y_train_resampled):\n",
    "        X_train_fold, X_val_fold = (\n",
    "            X_train_resampled.iloc[train_idx],\n",
    "            X_train_resampled.iloc[val_idx],\n",
    "        )\n",
    "        y_train_fold, y_val_fold = (\n",
    "            y_train_resampled.iloc[train_idx],\n",
    "            y_train_resampled.iloc[val_idx],\n",
    "        )\n",
    "\n",
    "        # Train Logistic Regression\n",
    "        try:\n",
    "            logger.info(f\"Fold {len(fold_aucs) + 1}: Training Logistic Regression...\")\n",
    "            model = LogisticRegression(\n",
    "                solver=solver,\n",
    "                penalty=penalty,\n",
    "                C=C,\n",
    "                tol=tol,\n",
    "                max_iter=max_iter,\n",
    "                l1_ratio=l1_ratio,\n",
    "                random_state=42,\n",
    "            )\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_prob = model.predict_proba(X_val_fold)[:, 1]\n",
    "            fold_auc = roc_auc_score(y_val_fold, y_prob)\n",
    "            fold_aucs.append(fold_auc)\n",
    "            logger.info(f\"Fold {len(fold_aucs)}: AUC-ROC = {fold_auc:.4f}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {e}. Pruning trial.\")\n",
    "            raise optuna.exceptions.TrialPruned()  # Handle invalid configurations\n",
    "\n",
    "    # Save fold AUCs for debugging\n",
    "    trial.set_user_attr(\"fold_aucs\", fold_aucs)\n",
    "    trial.set_user_attr(\"mean_auc\", np.mean(fold_aucs))\n",
    "    trial.set_user_attr(\"params\", trial.params)\n",
    "\n",
    "    # Generate predictions on the test set and save submission\n",
    "    test_probabilities = model.predict_proba(X_test)[:, 1]\n",
    "    submission = submission_template.copy()\n",
    "    submission[\"isFraud\"] = (\n",
    "        test_probabilities  # Assuming 'isFraud' is the positive class\n",
    "    )\n",
    "\n",
    "    submission_file = os.path.join(\n",
    "        output_folder, f\"submission_trial_{trial.number}.csv\"\n",
    "    )\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    logger.info(f\"Trial {trial.number}: Submission saved to {submission_file}\")\n",
    "\n",
    "    mean_auc = np.mean(fold_aucs)\n",
    "    logger.info(f\"Trial {trial.number}: Mean AUC-ROC across folds = {mean_auc:.4f}\")\n",
    "\n",
    "    return mean_auc\n",
    "\n",
    "\n",
    "# Run the optimization\n",
    "logger.info(\"Starting Bayesian optimization with Optuna...\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Save results\n",
    "all_trials = []\n",
    "for trial in study.trials:\n",
    "    all_trials.append(\n",
    "        {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"params\": trial.params,\n",
    "            \"mean_auc\": trial.user_attrs[\"mean_auc\"],\n",
    "            \"fold_aucs\": trial.user_attrs[\"fold_aucs\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(all_trials)\n",
    "results_file = os.path.join(output_folder, \"all_trials_results.csv\")\n",
    "results_df.to_csv(results_file, index=False)\n",
    "logger.info(f\"All trial results saved to {results_file}\")\n",
    "\n",
    "# Print and save top 3 models\n",
    "top_3 = results_df.sort_values(\"mean_auc\", ascending=False).head(3)\n",
    "logger.info(\"\\nTop 3 Models:\\n\" + top_3.to_string())\n",
    "\n",
    "top_3_file = os.path.join(output_folder, \"top_3_models.csv\")\n",
    "top_3.to_csv(top_3_file, index=False)\n",
    "logger.info(f\"Top 3 models saved to {top_3_file}\")\n",
    "\n",
    "# Save each trial's parameters and fold scores\n",
    "for trial in study.trials:\n",
    "    trial_details = {\n",
    "        \"params\": trial.params,\n",
    "        \"fold_aucs\": trial.user_attrs[\"fold_aucs\"],\n",
    "        \"mean_auc\": trial.user_attrs[\"mean_auc\"],\n",
    "    }\n",
    "    trial_file = os.path.join(output_folder, f\"trial_{trial.number}.json\")\n",
    "    with open(trial_file, \"w\") as f:\n",
    "        f.write(str(trial_details))\n",
    "    logger.info(f\"Details for Trial {trial.number} saved to {trial_file}\")\n",
    "\n",
    "# Save selected parameters and AUC scores for review\n",
    "selected_params_file = os.path.join(output_folder, \"selected_parameters.txt\")\n",
    "with open(selected_params_file, \"w\") as f:\n",
    "    for trial in study.trials:\n",
    "        f.write(\n",
    "            f\"Trial {trial.number}: Params: {trial.params}, Mean AUC: {trial.user_attrs['mean_auc']}, Fold AUCs: {trial.user_attrs['fold_aucs']}\\n\"\n",
    "        )\n",
    "logger.info(f\"Selected parameters and AUC scores saved to {selected_params_file}\")\n",
    "\n",
    "logger.info(f\"Optimization complete. Results saved in folder: {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import optuna\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create timestamped folder for saving results\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_folder = f\"catboost_bayesian_results_{timestamp}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "log_file = os.path.join(output_folder, \"log.txt\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    filemode=\"a\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for CatBoost\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1, 10),\n",
    "        \"bagging_temperature\": trial.suggest_uniform(\"bagging_temperature\", 0, 2),\n",
    "        \"random_strength\": trial.suggest_uniform(\"random_strength\", 0, 10),\n",
    "        \"scale_pos_weight\": trial.suggest_uniform(\"scale_pos_weight\", 1, 10),\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "    logger.info(f\"Trial {trial.number}: Starting with parameters: {params}\")\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_aucs = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        kf.split(X_train_resampled, y_train_resampled), 1\n",
    "    ):\n",
    "        logger.info(f\"  Fold {fold}: Training CatBoost...\")\n",
    "        X_train_fold, X_val_fold = (\n",
    "            X_train_resampled.iloc[train_idx],\n",
    "            X_train_resampled.iloc[val_idx],\n",
    "        )\n",
    "        y_train_fold, y_val_fold = (\n",
    "            y_train_resampled.iloc[train_idx],\n",
    "            y_train_resampled.iloc[val_idx],\n",
    "        )\n",
    "\n",
    "        # Train CatBoost Classifier\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            eval_set=(X_val_fold, y_val_fold),\n",
    "            early_stopping_rounds=50,\n",
    "        )\n",
    "\n",
    "        # Predict probabilities for validation set\n",
    "        y_prob = model.predict_proba(X_val_fold)[:, 1]\n",
    "        fold_auc = roc_auc_score(y_val_fold, y_prob)\n",
    "        fold_aucs.append(fold_auc)\n",
    "        logger.info(f\"  Fold {fold}: AUC-ROC = {fold_auc:.4f}\")\n",
    "\n",
    "    # Calculate mean AUC-ROC across folds\n",
    "    mean_auc = np.mean(fold_aucs)\n",
    "    logger.info(f\"Trial {trial.number}: Mean AUC-ROC across folds = {mean_auc:.4f}\")\n",
    "\n",
    "    # Save predictions on test set for this trial\n",
    "    test_probabilities = model.predict_proba(X_test)\n",
    "    submission = submission_template.copy()\n",
    "    submission[\"isFraud\"] = test_probabilities[:, 1]\n",
    "    submission_file = os.path.join(\n",
    "        output_folder, f\"submission_trial_{trial.number}.csv\"\n",
    "    )\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    logger.info(f\"Trial {trial.number}: Submission saved to {submission_file}\")\n",
    "\n",
    "    # Store results\n",
    "    trial.set_user_attr(\"fold_aucs\", fold_aucs)\n",
    "    trial.set_user_attr(\"mean_auc\", mean_auc)\n",
    "    trial.set_user_attr(\"params\", params)\n",
    "\n",
    "    return mean_auc\n",
    "\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "logger.info(\"Starting Bayesian optimization with Optuna...\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Save all trials results\n",
    "all_trials = []\n",
    "for trial in study.trials:\n",
    "    all_trials.append(\n",
    "        {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"params\": trial.params,\n",
    "            \"mean_auc\": trial.user_attrs[\"mean_auc\"],\n",
    "            \"fold_aucs\": trial.user_attrs[\"fold_aucs\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(all_trials)\n",
    "results_file = os.path.join(output_folder, \"all_trials_results.csv\")\n",
    "results_df.to_csv(results_file, index=False)\n",
    "logger.info(f\"All trial results saved to {results_file}\")\n",
    "\n",
    "# Print and save top 3 models\n",
    "top_3 = results_df.sort_values(\"mean_auc\", ascending=False).head(3)\n",
    "logger.info(\"\\nTop 3 Models:\\n\" + top_3.to_string())\n",
    "\n",
    "top_3_file = os.path.join(output_folder, \"top_3_models.csv\")\n",
    "top_3.to_csv(top_3_file, index=False)\n",
    "logger.info(f\"Top 3 models saved to {top_3_file}\")\n",
    "\n",
    "# Save detailed trial logs\n",
    "selected_params_file = os.path.join(output_folder, \"selected_parameters.txt\")\n",
    "with open(selected_params_file, \"w\") as f:\n",
    "    for trial in study.trials:\n",
    "        f.write(\n",
    "            f\"Trial {trial.number}: Params: {trial.params}, Mean AUC: {trial.user_attrs['mean_auc']}, Fold AUCs: {trial.user_attrs['fold_aucs']}\\n\"\n",
    "        )\n",
    "logger.info(f\"Selected parameters and AUC scores saved to {selected_params_file}\")\n",
    "\n",
    "logger.info(f\"Optimization complete. Results saved in folder: {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create timestamped folder for saving results\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_folder = f\"lgbm_bayesian_results_{timestamp}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space for LightGBM\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\n",
    "            \"learning_rate\", 0.01, 0.1\n",
    "        ),  # Reduced range for faster convergence\n",
    "        \"n_estimators\": trial.suggest_int(\n",
    "            \"n_estimators\", 100, 1000\n",
    "        ),  # Lower maximum iterations\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 10),  # Limit to smaller depths\n",
    "        \"num_leaves\": trial.suggest_int(\n",
    "            \"num_leaves\", 20, 50\n",
    "        ),  # Fewer leaves to reduce complexity\n",
    "        \"min_child_samples\": trial.suggest_int(\n",
    "            \"min_child_samples\", 20, 50\n",
    "        ),  # Moderate range for child samples\n",
    "        \"subsample\": trial.suggest_uniform(\n",
    "            \"subsample\", 0.7, 1.0\n",
    "        ),  # Keep subsampling high for better generalization\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\n",
    "            \"colsample_bytree\", 0.7, 1.0\n",
    "        ),  # Similar to subsample\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\n",
    "            \"reg_lambda\", 0.1, 5.0\n",
    "        ),  # Reduced range for regularization\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\n",
    "            \"reg_alpha\", 0.1, 5.0\n",
    "        ),  # Reduced range for regularization\n",
    "        \"scale_pos_weight\": trial.suggest_uniform(\n",
    "            \"scale_pos_weight\", 1, 5\n",
    "        ),  # Smaller range for class imbalance scaling\n",
    "    }\n",
    "\n",
    "    print(f\"\\nTrial {trial.number}: Starting with parameters: {params}\")\n",
    "\n",
    "    # Initialize LightGBM model\n",
    "    lgbm_model = LGBMClassifier(**params, random_state=42)\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_aucs = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        kf.split(X_train_resampled, y_train_resampled), 1\n",
    "    ):\n",
    "        print(f\"  Fold {fold}: Training LGBMClassifier...\")\n",
    "        X_train_fold, X_val_fold = (\n",
    "            X_train_resampled.iloc[train_idx],\n",
    "            X_train_resampled.iloc[val_idx],\n",
    "        )\n",
    "        y_train_fold, y_val_fold = (\n",
    "            y_train_resampled.iloc[train_idx],\n",
    "            y_train_resampled.iloc[val_idx],\n",
    "        )\n",
    "\n",
    "        # Train LightGBM model\n",
    "        lgbm_model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_metric=\"auc\",\n",
    "        )\n",
    "\n",
    "        # Predict probabilities for validation set\n",
    "        y_prob = lgbm_model.predict_proba(X_val_fold)[:, 1]\n",
    "        fold_auc = roc_auc_score(y_val_fold, y_prob)\n",
    "        fold_aucs.append(fold_auc)\n",
    "        print(f\"  Fold {fold}: AUC-ROC = {fold_auc:.4f}\")\n",
    "\n",
    "    # Calculate mean AUC-ROC across folds\n",
    "    mean_auc = np.mean(fold_aucs)\n",
    "    print(f\"Trial {trial.number}: Mean AUC-ROC across folds = {mean_auc:.4f}\")\n",
    "\n",
    "    # Save predictions on test set for this trial\n",
    "    test_probabilities = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "    submission = submission_template.copy()\n",
    "    submission[\"isFraud\"] = test_probabilities\n",
    "    submission_file = os.path.join(\n",
    "        output_folder, f\"submission_trial_{trial.number}.csv\"\n",
    "    )\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"Trial {trial.number}: Submission saved to {submission_file}\")\n",
    "\n",
    "    # Store results\n",
    "    trial.set_user_attr(\"fold_aucs\", fold_aucs)\n",
    "    trial.set_user_attr(\"mean_auc\", mean_auc)\n",
    "    trial.set_user_attr(\"params\", params)\n",
    "\n",
    "    return mean_auc\n",
    "\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)  # Adjust the number of trials as needed\n",
    "\n",
    "# Save all trials results\n",
    "all_trials = []\n",
    "for trial in study.trials:\n",
    "    all_trials.append(\n",
    "        {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"params\": trial.user_attrs[\"params\"],\n",
    "            \"mean_auc\": trial.user_attrs[\"mean_auc\"],\n",
    "            \"fold_aucs\": trial.user_attrs[\"fold_aucs\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(all_trials)\n",
    "results_file = os.path.join(output_folder, \"all_trials_results.csv\")\n",
    "results_df.to_csv(results_file, index=False)\n",
    "\n",
    "# Print and save top 3 models\n",
    "top_3 = results_df.sort_values(\"mean_auc\", ascending=False).head(3)\n",
    "print(\"\\nTop 3 Models:\")\n",
    "print(top_3)\n",
    "\n",
    "top_3_file = os.path.join(output_folder, \"top_3_models.csv\")\n",
    "top_3.to_csv(top_3_file, index=False)\n",
    "\n",
    "# Save detailed trial logs\n",
    "selected_params_file = os.path.join(output_folder, \"selected_parameters.txt\")\n",
    "with open(selected_params_file, \"w\") as f:\n",
    "    for trial in study.trials:\n",
    "        f.write(\n",
    "            f\"Trial {trial.number}: Params: {trial.user_attrs['params']}, Mean AUC: {trial.user_attrs['mean_auc']}, Fold AUCs: {trial.user_attrs['fold_aucs']}\\n\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nOptimization complete. Results saved in folder: {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
